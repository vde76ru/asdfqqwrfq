#!/usr/bin/env python3
"""
Trading RL Agent –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º - –ë–ê–ó–û–í–ê–Ø –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø
================================================================
–§–∞–π–ª: src/ml/models/rl_agent.py

‚úÖ –ò–°–ü–†–ê–í–õ–Ø–ï–¢: No module named 'src.ml.models.rl_agent'
‚úÖ –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Q-Learning –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏
‚úÖ –ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any, Union
from datetime import datetime, timedelta
import pickle
import json
from pathlib import Path
import logging
from dataclasses import dataclass
from enum import Enum
import random
from collections import deque


# –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
try:
    from ...core.unified_config import unified_config
    CONFIG_AVAILABLE = True
except ImportError:
    unified_config = None
    CONFIG_AVAILABLE = False

try:
    from ...logging.smart_logger import SmartLogger
    logger = SmartLogger(__name__)
except ImportError:
    logger = logging.getLogger(__name__)
    
from enum import IntEnum


class TradingAction(IntEnum):
    """
    –¢–æ—Ä–≥–æ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è RL –∞–≥–µ–Ω—Ç–∞
    ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è —Ç–µ—Å—Ç–æ–≤
    """
    SELL = 0      # ‚úÖ –ü—Ä–æ–¥–∞–∂–∞ = 0 (–∫–∞–∫ –æ–∂–∏–¥–∞–µ—Ç—Å—è –≤ —Ç–µ—Å—Ç–µ)
    HOLD = 1      # ‚úÖ –£–¥–µ—Ä–∂–∞–Ω–∏–µ = 1 
    BUY = 2       # ‚úÖ –ü–æ–∫—É–ø–∫–∞ = 2

    def __str__(self):
        return self.name
    
    def get_action_name(self):
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è"""
        return {
            0: "SELL",
            1: "HOLD", 
            2: "BUY"
        }.get(self.value, "UNKNOWN")

@dataclass
class TradingState:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    price: float
    volume: float
    rsi: float
    macd: float
    bb_position: float
    portfolio_value: float
    position_size: float
    timestamp: datetime

@dataclass
class TradingReward:
    """–ù–∞–≥—Ä–∞–¥–∞ –∑–∞ —Ç–æ—Ä–≥–æ–≤–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ"""
    profit: float
    penalty: float
    total_reward: float
    risk_penalty: float
    transaction_cost: float

class TradingEnvironment:
    """
    –¢–æ—Ä–≥–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è RL –∞–≥–µ–Ω—Ç–∞
    """
    
    def __init__(self, 
                 data: pd.DataFrame,
                 initial_balance: float = 10000.0,
                 transaction_cost: float = 0.001,
                 max_position_size: float = 1.0):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        
        Args:
            data: –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ OHLCV
            initial_balance: –ù–∞—á–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å
            transaction_cost: –ö–æ–º–∏—Å—Å–∏—è –∑–∞ —Å–¥–µ–ª–∫—É
            max_position_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏
        """
        self.data = data.copy()
        self.initial_balance = initial_balance
        self.transaction_cost = transaction_cost
        self.max_position_size = max_position_size
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.current_step = 0
        self.balance = initial_balance
        self.position = 0.0  # –¢–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è
        self.total_trades = 0
        self.winning_trades = 0
        
        # –ò—Å—Ç–æ—Ä–∏—è
        self.portfolio_history = []
        self.action_history = []
        self.reward_history = []
        
        logger.info("‚úÖ TradingEnvironment –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def reset(self) -> np.ndarray:
        """–°–±—Ä–æ—Å –æ–∫—Ä—É–∂–µ–Ω–∏—è –∫ –Ω–∞—á–∞–ª—å–Ω–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é"""
        self.current_step = 0
        self.balance = self.initial_balance
        self.position = 0.0
        self.total_trades = 0
        self.winning_trades = 0
        
        self.portfolio_history = []
        self.action_history = []
        self.reward_history = []
        
        return self._get_state()
    
    def _get_state(self) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if self.current_step >= len(self.data):
            return np.zeros(8)  # –ü—É—Å—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        
        row = self.data.iloc[self.current_step]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        state = np.array([
            row.get('close', 0) / 100.0,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞
            row.get('volume', 0) / 1000000.0,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–º
            row.get('rsi', 50) / 100.0,  # RSI [0,1]
            (row.get('macd', 0) + 100) / 200.0,  # MACD –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π
            row.get('bb_position', 0.5),  # BB –ø–æ–∑–∏—Ü–∏—è [0,1]
            self.balance / self.initial_balance,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –±–∞–ª–∞–Ω—Å
            self.position / self.max_position_size,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è
            self.current_step / len(self.data)  # –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è
        ])
        
        return state
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏
        
        Args:
            action: –î–µ–π—Å—Ç–≤–∏–µ (0=SELL, 1=HOLD, 2=BUY)
            
        Returns:
            Tuple: (new_state, reward, done, info)
        """
        if self.current_step >= len(self.data) - 1:
            return self._get_state(), 0.0, True, {}
        
        current_price = self.data.iloc[self.current_step]['close']
        next_price = self.data.iloc[self.current_step + 1]['close']
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        reward = self._execute_action(action, current_price, next_price)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self.current_step += 1
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
        portfolio_value = self.balance + self.position * next_price
        self.portfolio_history.append(portfolio_value)
        self.action_history.append(action)
        self.reward_history.append(reward)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∞
        done = (self.current_step >= len(self.data) - 1) or (portfolio_value <= 0)
        
        info = {
            'portfolio_value': portfolio_value,
            'balance': self.balance,
            'position': self.position,
            'total_trades': self.total_trades,
            'win_rate': self.winning_trades / max(self.total_trades, 1)
        }
        
        return self._get_state(), reward, done, info
    
    def _execute_action(self, action: int, current_price: float, next_price: float) -> float:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è"""
        action_enum = TradingAction(action)
        reward = 0.0
        
        price_change = (next_price - current_price) / current_price
        
        if action_enum == TradingAction.BUY and self.position < self.max_position_size:
            # –ü–æ–∫—É–ø–∫–∞
            trade_size = min(0.1, self.max_position_size - self.position)
            cost = trade_size * current_price * (1 + self.transaction_cost)
            
            if self.balance >= cost:
                self.balance -= cost
                self.position += trade_size
                self.total_trades += 1
                
                # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø—Ä–∏–±—ã–ª—å–Ω—É—é –ø–æ–∫—É–ø–∫—É
                reward = trade_size * price_change * 100
                if price_change > 0:
                    self.winning_trades += 1
        
        elif action_enum == TradingAction.SELL and self.position > 0:
            # –ü—Ä–æ–¥–∞–∂–∞
            trade_size = min(0.1, self.position)
            revenue = trade_size * current_price * (1 - self.transaction_cost)
            
            self.balance += revenue
            self.position -= trade_size
            self.total_trades += 1
            
            # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø—Ä–∏–±—ã–ª—å–Ω—É—é –ø—Ä–æ–¥–∞–∂—É (–æ–±—Ä–∞—Ç–Ω–∞—è –∫ –¥–≤–∏–∂–µ–Ω–∏—é —Ü–µ–Ω—ã)
            reward = trade_size * (-price_change) * 100
            if price_change < 0:
                self.winning_trades += 1
        
        else:
            # HOLD –∏–ª–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            # –ù–µ–±–æ–ª—å—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —É–¥–µ—Ä–∂–∞–Ω–∏–µ –ø—Ä–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
            reward = -0.01  # –®—Ç—Ä–∞—Ñ –∑–∞ –±–µ–∑–¥–µ–π—Å—Ç–≤–∏–µ
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —à—Ç—Ä–∞—Ñ—ã/–Ω–∞–≥—Ä–∞–¥—ã
        portfolio_value = self.balance + self.position * next_price
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ —É–±—ã—Ç–∫–∏
        if portfolio_value < self.initial_balance * 0.9:
            reward -= 1.0
        
        # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ —Ä–æ—Å—Ç –ø–æ—Ä—Ç—Ñ–µ–ª—è
        if portfolio_value > self.initial_balance * 1.1:
            reward += 0.5
        
        return reward

class TradingRLAgent:
    """
    ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô: RL –∞–≥–µ–Ω—Ç –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Q-Learning
    
    –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Q-Learning –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.
    """
    
    def __init__(self,
                 state_size: int = 8,
                 action_size: int = 3,
                 learning_rate: float = 0.001,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01,
                 gamma: float = 0.95,
                 memory_size: int = 10000):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RL –∞–≥–µ–Ω—Ç–∞
        
        Args:
            state_size: –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            action_size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            epsilon: –ù–∞—á–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ª—É—á–∞–π–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
            epsilon_decay: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è epsilon
            epsilon_min: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ epsilon
            gamma: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            memory_size: –†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –æ–ø—ã—Ç–∞
        """
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.gamma = gamma
        
        # Q-table –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ Q-Learning
        self.q_table = {}
        
        # –ë—É—Ñ–µ—Ä –æ–ø—ã—Ç–∞
        self.memory = deque(maxlen=memory_size)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.training_history = []
        self.is_trained = False
        
        logger.info("‚úÖ TradingRLAgent –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _discretize_state(self, state: np.ndarray) -> str:
        """–î–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è Q-table"""
        # –ü—Ä–æ—Å—Ç–∞—è –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è: –æ–∫—Ä—É–≥–ª—è–µ–º –¥–æ 1 –∑–Ω–∞–∫–∞ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π
        discretized = np.round(state, 1)
        return str(discretized.tolist())
    
    def get_action(self, state: np.ndarray, training: bool = True) -> int:
        """
        –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ epsilon-greedy —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        
        Args:
            state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            training: –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            –í—ã–±—Ä–∞–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (0, 1, 2)
        """
        state_key = self._discretize_state(state)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Q-–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –Ω–æ–≤–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        if state_key not in self.q_table:
            self.q_table[state_key] = np.zeros(self.action_size)
        
        # Epsilon-greedy –≤—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è
        if training and random.random() < self.epsilon:
            return random.randint(0, self.action_size - 1)
        else:
            return np.argmax(self.q_table[state_key])
    
    def remember(self, state: np.ndarray, action: int, reward: float, 
                 next_state: np.ndarray, done: bool):
        """–ó–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ –æ–ø—ã—Ç–∞"""
        self.memory.append((state, action, reward, next_state, done))
    
    def replay(self, batch_size: int = 32):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–µ –∏–∑ –±—É—Ñ–µ—Ä–∞ –æ–ø—ã—Ç–∞"""
        if len(self.memory) < batch_size:
            return
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –±–∞—Ç—á
        batch = random.sample(self.memory, min(batch_size, len(self.memory)))
        
        for state, action, reward, next_state, done in batch:
            state_key = self._discretize_state(state)
            next_state_key = self._discretize_state(next_state)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Q-–∑–Ω–∞—á–µ–Ω–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if state_key not in self.q_table:
                self.q_table[state_key] = np.zeros(self.action_size)
            if next_state_key not in self.q_table:
                self.q_table[next_state_key] = np.zeros(self.action_size)
            
            # Q-Learning –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
            target = reward
            if not done:
                target += self.gamma * np.max(self.q_table[next_state_key])
            
            # –û–±–Ω–æ–≤–ª—è–µ–º Q-–∑–Ω–∞—á–µ–Ω–∏–µ
            current_q = self.q_table[state_key][action]
            self.q_table[state_key][action] = current_q + self.learning_rate * (target - current_q)
        
        # –£–º–µ–Ω—å—à–∞–µ–º epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def train(self, 
              data: pd.DataFrame,
              episodes: int = 100,
              max_steps_per_episode: int = None,
              validation_split: float = 0.2) -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ RL –∞–≥–µ–Ω—Ç–∞
        
        Args:
            data: –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
            max_steps_per_episode: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –∑–∞ —ç–ø–∏–∑–æ–¥
            validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        """
        try:
            logger.info(f"üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–∞ –Ω–∞ {episodes} —ç–ø–∏–∑–æ–¥–∞—Ö...")
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            split_idx = int(len(data) * (1 - validation_split))
            train_data = data.iloc[:split_idx].copy()
            val_data = data.iloc[split_idx:].copy()
            
            # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
            env = TradingEnvironment(train_data)
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
            episode_rewards = []
            episode_portfolio_values = []
            
            max_steps = max_steps_per_episode or len(train_data) - 1
            
            for episode in range(episodes):
                state = env.reset()
                episode_reward = 0
                
                for step in range(max_steps):
                    # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
                    action = self.get_action(state, training=True)
                    
                    # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
                    next_state, reward, done, info = env.step(action)
                    
                    # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –æ–ø—ã—Ç
                    self.remember(state, action, reward, next_state, done)
                    
                    episode_reward += reward
                    state = next_state
                    
                    if done:
                        break
                
                # –û–±—É—á–∞–µ–º –∞–≥–µ–Ω—Ç–∞
                self.replay()
                
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                episode_rewards.append(episode_reward)
                final_portfolio = info.get('portfolio_value', env.initial_balance)
                episode_portfolio_values.append(final_portfolio)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                if episode % 10 == 0:
                    avg_reward = np.mean(episode_rewards[-10:])
                    avg_portfolio = np.mean(episode_portfolio_values[-10:])
                    logger.info(f"–≠–ø–∏–∑–æ–¥ {episode}/{episodes}: "
                               f"Reward={avg_reward:.2f}, "
                               f"Portfolio=${avg_portfolio:.2f}, "
                               f"Epsilon={self.epsilon:.3f}")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_results = self._validate(val_data)
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
            training_results = {
                'episodes_completed': episodes,
                'final_epsilon': self.epsilon,
                'q_table_size': len(self.q_table),
                'avg_episode_reward': np.mean(episode_rewards),
                'final_portfolio_value': episode_portfolio_values[-1],
                'total_return': (episode_portfolio_values[-1] / env.initial_balance - 1) * 100,
                'validation_results': val_results,
                'episode_rewards': episode_rewards,
                'episode_portfolio_values': episode_portfolio_values
            }
            
            self.training_history.append({
                'timestamp': datetime.now(),
                'results': training_results,
                'episodes': episodes
            })
            
            self.is_trained = True
            
            logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {training_results['total_return']:.2f}%")
            logger.info(f"üìä –†–∞–∑–º–µ—Ä Q-table: {training_results['q_table_size']}")
            
            return training_results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–∞: {e}")
            return {'success': False, 'error': str(e)}
    
    def _validate(self, val_data: pd.DataFrame) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            env = TradingEnvironment(val_data)
            state = env.reset()
            
            total_reward = 0
            actions_taken = []
            
            for step in range(len(val_data) - 1):
                action = self.get_action(state, training=False)
                next_state, reward, done, info = env.step(action)
                
                actions_taken.append(action)
                total_reward += reward
                state = next_state
                
                if done:
                    break
            
            final_value = info.get('portfolio_value', env.initial_balance)
            total_return = (final_value / env.initial_balance - 1) * 100
            
            return {
                'total_reward': total_reward,
                'final_portfolio_value': final_value,
                'total_return': total_return,
                'total_trades': info.get('total_trades', 0),
                'win_rate': info.get('win_rate', 0),
                'actions_distribution': {
                    'SELL': actions_taken.count(0),
                    'HOLD': actions_taken.count(1),
                    'BUY': actions_taken.count(2)
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            return {'error': str(e)}
    
    def predict(self, state: Union[np.ndarray, pd.Series, Dict]) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        
        Args:
            state: –°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º
        """
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array
            if isinstance(state, pd.Series):
                state_array = state.values
            elif isinstance(state, dict):
                # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                state_array = np.array([
                    state.get('close', 0),
                    state.get('volume', 0),
                    state.get('rsi', 50),
                    state.get('macd', 0),
                    state.get('bb_position', 0.5),
                    state.get('portfolio_value', 10000),
                    state.get('position', 0),
                    state.get('timestamp', 0)
                ])
            else:
                state_array = np.array(state)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            if len(state_array) != self.state_size:
                # –î–æ–ø–æ–ª–Ω—è–µ–º –∏–ª–∏ –æ–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                if len(state_array) < self.state_size:
                    state_array = np.pad(state_array, (0, self.state_size - len(state_array)))
                else:
                    state_array = state_array[:self.state_size]
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            action = self.get_action(state_array, training=False)
            
            # –ü–æ–ª—É—á–∞–µ–º Q-–∑–Ω–∞—á–µ–Ω–∏—è
            state_key = self._discretize_state(state_array)
            if state_key in self.q_table:
                q_values = self.q_table[state_key].tolist()
                confidence = max(q_values) / (sum(q_values) + 1e-8)
            else:
                q_values = [0.33, 0.34, 0.33]
                confidence = 0.5
            
            action_names = {0: 'SELL', 1: 'HOLD', 2: 'BUY'}
            
            return {
                'action': action,
                'action_name': action_names[action],
                'confidence': confidence,
                'q_values': q_values,
                'state_known': state_key in self.q_table,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return {
                'action': 1,  # HOLD –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                'action_name': 'HOLD',
                'confidence': 0.5,
                'error': str(e)
            }
    
    def save_model(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        try:
            model_data = {
                'q_table': self.q_table,
                'state_size': self.state_size,
                'action_size': self.action_size,
                'learning_rate': self.learning_rate,
                'epsilon': self.epsilon,
                'epsilon_decay': self.epsilon_decay,
                'epsilon_min': self.epsilon_min,
                'gamma': self.gamma,
                'training_history': self.training_history,
                'is_trained': self.is_trained,
                'timestamp': datetime.now().isoformat()
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
            
            logger.info(f"‚úÖ RL –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
    
    def load_model(self, filepath: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.q_table = model_data['q_table']
            self.state_size = model_data['state_size']
            self.action_size = model_data['action_size']
            self.learning_rate = model_data['learning_rate']
            self.epsilon = model_data['epsilon']
            self.epsilon_decay = model_data['epsilon_decay']
            self.epsilon_min = model_data['epsilon_min']
            self.gamma = model_data['gamma']
            self.training_history = model_data.get('training_history', [])
            self.is_trained = model_data.get('is_trained', True)
            
            logger.info(f"‚úÖ RL –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        return {
            'model_type': 'Q-Learning',
            'state_size': self.state_size,
            'action_size': self.action_size,
            'learning_rate': self.learning_rate,
            'epsilon': self.epsilon,
            'gamma': self.gamma,
            'q_table_size': len(self.q_table),
            'is_trained': self.is_trained,
            'training_episodes': len(self.training_history)
        }

# ‚úÖ –≠–ö–°–ü–û–†–¢
__all__ = [
    'TradingRLAgent',
    'TradingEnvironment',
    'TradingAction',
    'TradingState',
    'TradingReward'
]