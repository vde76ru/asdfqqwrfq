#!/usr/bin/env python3
"""
Direction Classifier –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ü–µ–Ω—ã - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
==========================================================================
–§–∞–π–ª: src/ml/models/direction_classifier.py

‚úÖ –ò–°–ü–†–ê–í–õ–Ø–ï–¢: No module named 'src.ml.models.direction_classifier'
‚úÖ –ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π
‚úÖ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Union
from datetime import datetime, timedelta
import pickle
import json
from pathlib import Path
import logging

# –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã ML –±–∏–±–ª–∏–æ—Ç–µ–∫
try:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.pipeline import Pipeline
    SKLEARN_AVAILABLE = True
except ImportError:
    logger.warning("‚ö†Ô∏è scikit-learn –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    SKLEARN_AVAILABLE = False

try:
    from ...core.unified_config import unified_config
    CONFIG_AVAILABLE = True
except ImportError:
    unified_config = None
    CONFIG_AVAILABLE = False

try:
    from ...logging.smart_logger import SmartLogger
    logger = SmartLogger(__name__)
except ImportError:
    logger = logging.getLogger(__name__)

class DirectionClassifier:
    """
    ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
    
    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –±—É–¥—É—â–µ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã (UP/DOWN/SIDEWAYS)
    –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    """
    
    def __init__(self, 
                 model_type: str = 'random_forest',
                 forecast_horizon: int = 5,
                 threshold: float = 0.01,
                 confidence_threshold: float = 0.6):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        
        Args:
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ ('random_forest', 'gradient_boosting', 'logistic', 'svm', 'ensemble')
            forecast_horizon: –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–∏–æ–¥–æ–≤)
            threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ —Ü–µ–Ω—ã –¥–ª—è UP/DOWN (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
            confidence_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
        """
        self.model_type = model_type
        self.forecast_horizon = forecast_horizon
        self.threshold = threshold
        self.confidence_threshold = confidence_threshold
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model = None
        self.scaler = StandardScaler() if SKLEARN_AVAILABLE else None
        self.label_encoder = LabelEncoder() if SKLEARN_AVAILABLE else None
        self.feature_names: List[str] = []
        self.is_fitted = False
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.performance_metrics = {}
        self.training_history = []
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.model_params = self._get_default_params()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        if SKLEARN_AVAILABLE:
            self._initialize_model()
        
        logger.info(f"‚úÖ DirectionClassifier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {model_type}")
    
    def _get_default_params(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        params = {
            'random_forest': {
                'n_estimators': 100,
                'max_depth': 10,
                'min_samples_split': 5,
                'min_samples_leaf': 2,
                'random_state': 42
            },
            'gradient_boosting': {
                'n_estimators': 100,
                'learning_rate': 0.1,
                'max_depth': 6,
                'random_state': 42
            },
            'logistic': {
                'max_iter': 1000,
                'random_state': 42,
                'solver': 'liblinear'
            },
            'svm': {
                'kernel': 'rbf',
                'probability': True,
                'random_state': 42
            },
            'naive_bayes': {}
        }
        
        return params.get(self.model_type, {})
    
    def _initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ML"""
        if not SKLEARN_AVAILABLE:
            logger.error("‚ùå scikit-learn –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return
        
        try:
            if self.model_type == 'random_forest':
                self.model = RandomForestClassifier(**self.model_params)
            elif self.model_type == 'gradient_boosting':
                self.model = GradientBoostingClassifier(**self.model_params)
            elif self.model_type == 'logistic':
                self.model = LogisticRegression(**self.model_params)
            elif self.model_type == 'svm':
                self.model = SVC(**self.model_params)
            elif self.model_type == 'naive_bayes':
                self.model = GaussianNB(**self.model_params)
            elif self.model_type == 'ensemble':
                self._initialize_ensemble()
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {self.model_type}, –∏—Å–ø–æ–ª—å–∑—É–µ–º Random Forest")
                self.model = RandomForestClassifier(**self.model_params['random_forest'])
                self.model_type = 'random_forest'
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {self.model_type} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
            self.model = RandomForestClassifier(n_estimators=50, random_state=42)
    
    def _initialize_ensemble(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
        try:
            from sklearn.ensemble import VotingClassifier
            
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
            models = [
                ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
                ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42)),
                ('lr', LogisticRegression(max_iter=1000, random_state=42))
            ]
            
            self.model = VotingClassifier(
                estimators=models,
                voting='soft'  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            )
            
            logger.info("‚úÖ –ê–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è: {e}")
            self.model = RandomForestClassifier(n_estimators=50, random_state=42)
    
    def prepare_labels(self, df: pd.DataFrame, price_column: str = 'close') -> np.ndarray:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            df: DataFrame —Å —Ü–µ–Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            price_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ü–µ–Ω–æ–π
            
        Returns:
            –ú–∞—Å—Å–∏–≤ –º–µ—Ç–æ–∫ (0=DOWN, 1=SIDEWAYS, 2=UP)
        """
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –±—É–¥—É—â–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã
            future_returns = df[price_column].pct_change(periods=self.forecast_horizon).shift(-self.forecast_horizon)
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤
            labels = np.where(
                future_returns > self.threshold, 2,  # UP
                np.where(future_returns < -self.threshold, 0, 1)  # DOWN –∏–ª–∏ SIDEWAYS
            )
            
            # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –Ω–µ—Ç –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            valid_labels = labels[:-self.forecast_horizon]
            
            logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(valid_labels)} –º–µ—Ç–æ–∫")
            logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: UP={np.sum(valid_labels==2)}, "
                       f"SIDEWAYS={np.sum(valid_labels==1)}, DOWN={np.sum(valid_labels==0)}")
            
            return valid_labels
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –º–µ—Ç–æ–∫: {e}")
            return np.array([])
    
    def train(self, 
              features_df: pd.DataFrame, 
              price_column: str = 'close',
              test_size: float = 0.2,
              validate: bool = True) -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Args:
            features_df: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ —Ü–µ–Ω–∞–º–∏
            price_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ü–µ–Ω–æ–π
            test_size: –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
            validate: –ü—Ä–æ–≤–æ–¥–∏—Ç—å –ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        if not SKLEARN_AVAILABLE:
            logger.error("‚ùå scikit-learn –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return {'error': 'sklearn_unavailable'}
        
        try:
            logger.info("üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è DirectionClassifier...")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_columns = [col for col in features_df.columns 
                             if col not in ['timestamp', 'open', 'high', 'low', 'close', 'volume']]
            
            if not feature_columns:
                logger.error("‚ùå –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return {'error': 'no_features'}
            
            self.feature_names = feature_columns
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X = features_df[feature_columns].copy()
            y = self.prepare_labels(features_df, price_column)
            
            if len(y) == 0:
                logger.error("‚ùå –ù–µ—Ç –º–µ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return {'error': 'no_labels'}
            
            # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –±—É–¥—É—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X = X.iloc[:-self.forecast_horizon]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤
            if len(X) != len(y):
                min_len = min(len(X), len(y))
                X = X.iloc[:min_len]
                y = y[:min_len]
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
            valid_indices = ~(X.isnull().any(axis=1) | pd.isnull(y))
            X = X[valid_indices]
            y = y[valid_indices]
            
            if len(X) == 0:
                logger.error("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏")
                return {'error': 'no_valid_data'}
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X_scaled = self.scaler.fit_transform(X)
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=test_size, random_state=42, stratify=y
            )
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            start_time = datetime.now()
            self.model.fit(X_train, y_train)
            training_time = (datetime.now() - start_time).total_seconds()
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred_train = self.model.predict(X_train)
            y_pred_test = self.model.predict(X_test)
            
            # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if hasattr(self.model, 'predict_proba'):
                y_proba_test = self.model.predict_proba(X_test)
                confidence = np.max(y_proba_test, axis=1).mean()
            else:
                confidence = 0.0
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            train_accuracy = accuracy_score(y_train, y_pred_train)
            test_accuracy = accuracy_score(y_test, y_pred_test)
            
            metrics = {
                'train_accuracy': train_accuracy,
                'test_accuracy': test_accuracy,
                'precision': precision_score(y_test, y_pred_test, average='weighted', zero_division=0),
                'recall': recall_score(y_test, y_pred_test, average='weighted', zero_division=0),
                'f1_score': f1_score(y_test, y_pred_test, average='weighted', zero_division=0),
                'confidence': confidence,
                'training_time': training_time,
                'train_samples': len(X_train),
                'test_samples': len(X_test),
                'features_count': len(feature_columns),
                'class_distribution': {
                    'UP': int(np.sum(y == 2)),
                    'SIDEWAYS': int(np.sum(y == 1)),
                    'DOWN': int(np.sum(y == 0))
                }
            }
            
            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            if validate and len(X) > 100:
                try:
                    cv_scores = cross_val_score(self.model, X_scaled, y, cv=5, scoring='accuracy')
                    metrics['cv_mean'] = cv_scores.mean()
                    metrics['cv_std'] = cv_scores.std()
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
                    metrics['cv_mean'] = 0.0
                    metrics['cv_std'] = 0.0
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            self.performance_metrics = metrics
            self.is_fitted = True
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            self.training_history.append({
                'timestamp': datetime.now(),
                'metrics': metrics,
                'model_type': self.model_type,
                'samples_count': len(X)
            })
            
            logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            logger.info(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {test_accuracy:.3f}")
            logger.info(f"üìä F1-score: {metrics['f1_score']:.3f}")
            logger.info(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f}")
            
            return metrics
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return {'error': str(e)}
    
    def predict(self, features_df: pd.DataFrame) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
        
        Args:
            features_df: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
        """
        if not self.is_fitted:
            logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return {'error': 'model_not_fitted'}
        
        if not SKLEARN_AVAILABLE:
            logger.error("‚ùå scikit-learn –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return {'error': 'sklearn_unavailable'}
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            missing_features = set(self.feature_names) - set(features_df.columns)
            if missing_features:
                logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
                return {'error': f'missing_features: {missing_features}'}
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X = features_df[self.feature_names].copy()
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
            valid_indices = ~X.isnull().any(axis=1)
            if not valid_indices.any():
                logger.error("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
                return {'error': 'no_valid_data'}
            
            X_valid = X[valid_indices]
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
            X_scaled = self.scaler.transform(X_valid)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            predictions = self.model.predict(X_scaled)
            
            # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            if hasattr(self.model, 'predict_proba'):
                probabilities = self.model.predict_proba(X_scaled)
                confidence = np.max(probabilities, axis=1)
            else:
                probabilities = None
                confidence = np.ones(len(predictions)) * 0.5
            
            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            direction_names = {0: 'DOWN', 1: 'SIDEWAYS', 2: 'UP'}
            direction_labels = [direction_names[pred] for pred in predictions]
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            high_confidence_mask = confidence >= self.confidence_threshold
            
            results = {
                'predictions': predictions.tolist(),
                'direction_labels': direction_labels,
                'confidence': confidence.tolist(),
                'probabilities': probabilities.tolist() if probabilities is not None else None,
                'high_confidence_count': int(np.sum(high_confidence_mask)),
                'total_predictions': len(predictions),
                'forecast_horizon': self.forecast_horizon,
                'threshold': self.threshold,
                'timestamp': datetime.now().isoformat(),
                'valid_indices': valid_indices.tolist()
            }
            
            # –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            unique, counts = np.unique(predictions, return_counts=True)
            direction_distribution = {direction_names[label]: int(count) 
                                   for label, count in zip(unique, counts)}
            results['direction_distribution'] = direction_distribution
            
            logger.info(f"‚úÖ –°–¥–µ–ª–∞–Ω–æ {len(predictions)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
            logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {direction_distribution}")
            logger.info(f"üìä –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {int(np.sum(high_confidence_mask))}/{len(predictions)}")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return {'error': str(e)}
    
    def predict_single(self, features: Union[pd.Series, Dict, np.ndarray]) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            features: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º
        """
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
            if isinstance(features, dict):
                features_df = pd.DataFrame([features])
            elif isinstance(features, pd.Series):
                features_df = features.to_frame().T
            elif isinstance(features, np.ndarray):
                features_df = pd.DataFrame([features], columns=self.feature_names)
            else:
                features_df = pd.DataFrame([features])
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            result = self.predict(features_df)
            
            if 'error' in result:
                return result
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
            single_result = {
                'prediction': result['predictions'][0],
                'direction': result['direction_labels'][0],
                'confidence': result['confidence'][0],
                'probabilities': result['probabilities'][0] if result['probabilities'] else None,
                'timestamp': result['timestamp']
            }
            
            return single_result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞: {e}")
            return {'error': str(e)}
    
    def evaluate(self, features_df: pd.DataFrame, price_column: str = 'close') -> Dict[str, Any]:
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            features_df: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ —Ü–µ–Ω–∞–º–∏
            price_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ü–µ–Ω–æ–π
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ—Ü–µ–Ω–∫–∏
        """
        if not self.is_fitted:
            logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return {'error': 'model_not_fitted'}
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X = features_df[self.feature_names].copy()
            y_true = self.prepare_labels(features_df, price_column)
            
            if len(y_true) == 0:
                return {'error': 'no_labels'}
            
            # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –±—É–¥—É—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            X = X.iloc[:-self.forecast_horizon]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã
            min_len = min(len(X), len(y_true))
            X = X.iloc[:min_len]
            y_true = y_true[:min_len]
            
            # –£–¥–∞–ª—è–µ–º NaN
            valid_indices = ~(X.isnull().any(axis=1) | pd.isnull(y_true))
            X = X[valid_indices]
            y_true = y_true[valid_indices]
            
            if len(X) == 0:
                return {'error': 'no_valid_data'}
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            X_scaled = self.scaler.transform(X)
            y_pred = self.model.predict(X_scaled)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            metrics = {
                'accuracy': accuracy_score(y_true, y_pred),
                'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),
                'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),
                'f1_score': f1_score(y_true, y_pred, average='weighted', zero_division=0),
                'samples_count': len(X),
                'timestamp': datetime.now().isoformat()
            }
            
            # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
            try:
                cm = confusion_matrix(y_true, y_pred)
                metrics['confusion_matrix'] = cm.tolist()
            except Exception:
                pass
            
            logger.info(f"‚úÖ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —Ç–æ—á–Ω–æ—Å—Ç—å {metrics['accuracy']:.3f}")
            return metrics
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏: {e}")
            return {'error': str(e)}
    
    def get_feature_importance(self) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if not self.is_fitted:
            logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return pd.DataFrame()
        
        try:
            if hasattr(self.model, 'feature_importances_'):
                importance = pd.DataFrame({
                    'feature': self.feature_names,
                    'importance': self.model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                return importance
            else:
                logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç feature_importances_")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return pd.DataFrame()
    
    def save_model(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        try:
            model_data = {
                'model': self.model,
                'scaler': self.scaler,
                'feature_names': self.feature_names,
                'model_type': self.model_type,
                'forecast_horizon': self.forecast_horizon,
                'threshold': self.threshold,
                'confidence_threshold': self.confidence_threshold,
                'performance_metrics': self.performance_metrics,
                'training_history': self.training_history,
                'is_fitted': self.is_fitted,
                'timestamp': datetime.now().isoformat()
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
    
    def load_model(self, filepath: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.model = model_data['model']
            self.scaler = model_data['scaler']
            self.feature_names = model_data['feature_names']
            self.model_type = model_data['model_type']
            self.forecast_horizon = model_data['forecast_horizon']
            self.threshold = model_data['threshold']
            self.confidence_threshold = model_data['confidence_threshold']
            self.performance_metrics = model_data.get('performance_metrics', {})
            self.training_history = model_data.get('training_history', [])
            self.is_fitted = model_data.get('is_fitted', True)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        return {
            'model_type': self.model_type,
            'forecast_horizon': self.forecast_horizon,
            'threshold': self.threshold,
            'confidence_threshold': self.confidence_threshold,
            'is_fitted': self.is_fitted,
            'features_count': len(self.feature_names),
            'feature_names': self.feature_names,
            'performance_metrics': self.performance_metrics,
            'training_history_count': len(self.training_history),
            'sklearn_available': SKLEARN_AVAILABLE
        }

# ‚úÖ –õ–ï–ì–ö–ê–Ø –í–ï–†–°–ò–Ø –î–õ–Ø –¢–ï–°–¢–û–í–û–ì–û –†–ï–ñ–ò–ú–ê
class DirectionClassifierLight:
    """–õ–µ–≥–∫–∞—è –≤–µ—Ä—Å–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞"""
    
    def __init__(self, **kwargs):
        self.is_fitted = False
        self.feature_names = []
        
    def train(self, *args, **kwargs):
        self.is_fitted = True
        return {'accuracy': 0.65, 'test_mode': True}
    
    def predict(self, features_df):
        size = len(features_df)
        return {
            'predictions': [1] * size,
            'direction_labels': ['SIDEWAYS'] * size,
            'confidence': [0.6] * size,
            'test_mode': True
        }
    
    def predict_single(self, features):
        return {
            'prediction': 1,
            'direction': 'SIDEWAYS',
            'confidence': 0.6,
            'test_mode': True
        }

# ‚úÖ –≠–ö–°–ü–û–†–¢
__all__ = [
    'DirectionClassifier',
    'DirectionClassifierLight'
]