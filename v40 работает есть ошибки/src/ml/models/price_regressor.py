#!/usr/bin/env python3
"""
Price Level Regressor –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
==========================================================================
–§–∞–π–ª: src/ml/models/price_regressor.py

‚úÖ –ò–°–ü–†–ê–í–õ–Ø–ï–¢: No module named 'src.ml.models.price_regressor'
‚úÖ –ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π
‚úÖ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è —Ü–µ–Ω
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Union
from datetime import datetime, timedelta
import pickle
import json
from pathlib import Path
import logging

# –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã ML –±–∏–±–ª–∏–æ—Ç–µ–∫
try:
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.linear_model import LinearRegression, Ridge, Lasso
    from sklearn.svm import SVR
    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.pipeline import Pipeline
    import joblib
    SKLEARN_AVAILABLE = True
except ImportError:
    logger.warning("‚ö†Ô∏è scikit-learn –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    SKLEARN_AVAILABLE = False

try:
    from ...core.unified_config import unified_config
    CONFIG_AVAILABLE = True
except ImportError:
    unified_config = None
    CONFIG_AVAILABLE = False

try:
    from ...logging.smart_logger import SmartLogger
    logger = SmartLogger(__name__)
except ImportError:
    logger = logging.getLogger(__name__)

class PriceLevelRegressor:
    """
    ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô: –†–µ–≥—Ä–µ—Å—Å–æ—Ä –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π
    
    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–µ–Ω–æ–≤—ã–µ —É—Ä–æ–≤–Ω–∏, –≤–∫–ª—é—á–∞—è:
    - –ë—É–¥—É—â—É—é —Ü–µ–Ω—É –∑–∞–∫—Ä—ã—Ç–∏—è
    - –£—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
    - –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ Take Profit –∏ Stop Loss —É—Ä–æ–≤–Ω–∏
    - –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –∏ –¥–∏–∞–ø–∞–∑–æ–Ω—ã —Ü–µ–Ω
    """
    
    def __init__(self, 
                 model_type: str = 'random_forest',
                 forecast_horizon: int = 5,
                 price_targets: List[str] = None,
                 scaling_method: str = 'standard'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–∞ —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π
        
        Args:
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ ('random_forest', 'gradient_boosting', 'linear', 'ridge', 'svr')
            forecast_horizon: –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–∏–æ–¥–æ–≤)
            price_targets: –°–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            scaling_method: –ú–µ—Ç–æ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è ('standard', 'minmax', 'none')
        """
        self.model_type = model_type
        self.forecast_horizon = forecast_horizon
        self.scaling_method = scaling_method
        
        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if price_targets is None:
            self.price_targets = ['future_close', 'support_level', 'resistance_level', 'volatility']
        else:
            self.price_targets = price_targets
        
        # –ú–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        self.models = {}
        self.scalers = {
            'features': self._get_scaler(),
            'targets': {}
        }
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.feature_names: List[str] = []
        self.is_fitted = False
        self.performance_metrics = {}
        self.training_history = []
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.model_params = self._get_default_params()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        if SKLEARN_AVAILABLE:
            self._initialize_models()
        
        logger.info(f"‚úÖ PriceLevelRegressor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {model_type}")
        logger.info(f"üìä –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {self.price_targets}")
    
    def _get_scaler(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–∫–∞–ª–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
        if not SKLEARN_AVAILABLE:
            return None
        
        if self.scaling_method == 'standard':
            return StandardScaler()
        elif self.scaling_method == 'minmax':
            return MinMaxScaler()
        else:
            return None
    
    def _get_default_params(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        params = {
            'random_forest': {
                'n_estimators': 100,
                'max_depth': 15,
                'min_samples_split': 5,
                'min_samples_leaf': 2,
                'random_state': 42
            },
            'gradient_boosting': {
                'n_estimators': 100,
                'learning_rate': 0.1,
                'max_depth': 8,
                'random_state': 42
            },
            'linear': {
                'fit_intercept': True
            },
            'ridge': {
                'alpha': 1.0,
                'random_state': 42
            },
            'lasso': {
                'alpha': 1.0,
                'random_state': 42,
                'max_iter': 1000
            },
            'svr': {
                'kernel': 'rbf',
                'C': 1.0,
                'epsilon': 0.1
            }
        }
        
        return params.get(self.model_type, {})
    
    def _get_model_instance(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –º–æ–¥–µ–ª–∏"""
        if not SKLEARN_AVAILABLE:
            return None
        
        if self.model_type == 'random_forest':
            return RandomForestRegressor(**self.model_params)
        elif self.model_type == 'gradient_boosting':
            return GradientBoostingRegressor(**self.model_params)
        elif self.model_type == 'linear':
            return LinearRegression(**self.model_params)
        elif self.model_type == 'ridge':
            return Ridge(**self.model_params)
        elif self.model_type == 'lasso':
            return Lasso(**self.model_params)
        elif self.model_type == 'svr':
            return SVR(**self.model_params)
        else:
            logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {self.model_type}, –∏—Å–ø–æ–ª—å–∑—É–µ–º Random Forest")
            return RandomForestRegressor(n_estimators=50, random_state=42)
    
    def _initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
        try:
            for target in self.price_targets:
                self.models[target] = self._get_model_instance()
                if self.scaling_method != 'none':
                    self.scalers['targets'][target] = self._get_scaler()
            
            logger.info(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(self.models)} –º–æ–¥–µ–ª–µ–π")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π: {e}")
    
    def prepare_targets(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        
        Args:
            df: DataFrame —Å —Ü–µ–Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (OHLCV)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
        """
        try:
            targets = {}
            
            # –ë—É–¥—É—â–∞—è —Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è
            if 'future_close' in self.price_targets:
                targets['future_close'] = df['close'].shift(-self.forecast_horizon).values
            
            # –ë—É–¥—É—â–∏–π –º–∞–∫—Å–∏–º—É–º (—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ)
            if 'resistance_level' in self.price_targets:
                targets['resistance_level'] = df['high'].rolling(
                    window=self.forecast_horizon
                ).max().shift(-self.forecast_horizon).values
            
            # –ë—É–¥—É—â–∏–π –º–∏–Ω–∏–º—É–º (–ø–æ–¥–¥–µ—Ä–∂–∫–∞)
            if 'support_level' in self.price_targets:
                targets['support_level'] = df['low'].rolling(
                    window=self.forecast_horizon
                ).min().shift(-self.forecast_horizon).values
            
            # –ë—É–¥—É—â–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            if 'volatility' in self.price_targets:
                future_returns = df['close'].pct_change().shift(-self.forecast_horizon)
                targets['volatility'] = future_returns.rolling(
                    window=self.forecast_horizon
                ).std().values
            
            # –ë—É–¥—É—â–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω
            if 'price_range' in self.price_targets:
                future_high = df['high'].shift(-self.forecast_horizon)
                future_low = df['low'].shift(-self.forecast_horizon)
                targets['price_range'] = (future_high - future_low).values
            
            # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π Take Profit —É—Ä–æ–≤–µ–Ω—å (–ø—Ä–∏–º–µ—Ä–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
            if 'optimal_tp' in self.price_targets:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—É–¥—É—â–∏–π –º–∞–∫—Å–∏–º—É–º –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ TP
                future_max = df['high'].rolling(
                    window=self.forecast_horizon*2
                ).max().shift(-self.forecast_horizon)
                targets['optimal_tp'] = (future_max / df['close'] - 1).values * 100
            
            # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π Stop Loss —É—Ä–æ–≤–µ–Ω—å
            if 'optimal_sl' in self.price_targets:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—É–¥—É—â–∏–π –º–∏–Ω–∏–º—É–º –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ SL
                future_min = df['low'].rolling(
                    window=self.forecast_horizon*2
                ).min().shift(-self.forecast_horizon)
                targets['optimal_sl'] = (df['close'] / future_min - 1).values * 100
            
            logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(targets)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
            
            return targets
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {e}")
            return {}
    
    def train(self, 
              features_df: pd.DataFrame,
              test_size: float = 0.2,
              validate: bool = True) -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        
        Args:
            features_df: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ —Ü–µ–Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            test_size: –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
            validate: –ü—Ä–æ–≤–æ–¥–∏—Ç—å –ª–∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        """
        if not SKLEARN_AVAILABLE:
            logger.error("‚ùå scikit-learn –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return {'error': 'sklearn_unavailable'}
        
        try:
            logger.info("üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è PriceLevelRegressor...")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_columns = [col for col in features_df.columns 
                             if col not in ['timestamp', 'open', 'high', 'low', 'close', 'volume']]
            
            if not feature_columns:
                logger.error("‚ùå –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return {'error': 'no_features'}
            
            self.feature_names = feature_columns
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            targets_dict = self.prepare_targets(features_df)
            if not targets_dict:
                logger.error("‚ùå –ù–µ—Ç —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
                return {'error': 'no_targets'}
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X = features_df[feature_columns].copy()
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –±—É–¥—É—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            valid_length = len(X) - self.forecast_horizon
            X = X.iloc[:valid_length]
            
            # –û–±—Ä–µ–∑–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–æ —Ç–æ–π –∂–µ –¥–ª–∏–Ω—ã
            for target_name in targets_dict:
                targets_dict[target_name] = targets_dict[target_name][:valid_length]
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
            valid_mask = ~X.isnull().any(axis=1)
            for target_name, target_values in targets_dict.items():
                valid_mask &= ~pd.isnull(target_values)
            
            X = X[valid_mask]
            for target_name in targets_dict:
                targets_dict[target_name] = targets_dict[target_name][valid_mask]
            
            if len(X) == 0:
                logger.error("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏")
                return {'error': 'no_valid_data'}
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if self.scalers['features']:
                X_scaled = self.scalers['features'].fit_transform(X)
            else:
                X_scaled = X.values
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            all_metrics = {}
            start_time = datetime.now()
            
            for target_name, y in targets_dict.items():
                if target_name not in self.models:
                    logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –¥–ª—è {target_name} –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
                    continue
                
                try:
                    logger.info(f"üîÑ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {target_name}...")
                    
                    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
                    if target_name in self.scalers['targets']:
                        y_scaled = self.scalers['targets'][target_name].fit_transform(
                            y.reshape(-1, 1)
                        ).ravel()
                    else:
                        y_scaled = y
                    
                    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç
                    X_train, X_test, y_train, y_test = train_test_split(
                        X_scaled, y_scaled, test_size=test_size, random_state=42
                    )
                    
                    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    self.models[target_name].fit(X_train, y_train)
                    
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                    y_pred_train = self.models[target_name].predict(X_train)
                    y_pred_test = self.models[target_name].predict(X_test)
                    
                    # –û–±—Ä–∞—Ç–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –º–µ—Ç—Ä–∏–∫
                    if target_name in self.scalers['targets']:
                        y_train_orig = self.scalers['targets'][target_name].inverse_transform(
                            y_train.reshape(-1, 1)
                        ).ravel()
                        y_test_orig = self.scalers['targets'][target_name].inverse_transform(
                            y_test.reshape(-1, 1)
                        ).ravel()
                        y_pred_train_orig = self.scalers['targets'][target_name].inverse_transform(
                            y_pred_train.reshape(-1, 1)
                        ).ravel()
                        y_pred_test_orig = self.scalers['targets'][target_name].inverse_transform(
                            y_pred_test.reshape(-1, 1)
                        ).ravel()
                    else:
                        y_train_orig = y_train
                        y_test_orig = y_test
                        y_pred_train_orig = y_pred_train
                        y_pred_test_orig = y_pred_test
                    
                    # –ú–µ—Ç—Ä–∏–∫–∏
                    metrics = {
                        'train_mse': mean_squared_error(y_train_orig, y_pred_train_orig),
                        'test_mse': mean_squared_error(y_test_orig, y_pred_test_orig),
                        'train_mae': mean_absolute_error(y_train_orig, y_pred_train_orig),
                        'test_mae': mean_absolute_error(y_test_orig, y_pred_test_orig),
                        'train_r2': r2_score(y_train_orig, y_pred_train_orig),
                        'test_r2': r2_score(y_test_orig, y_pred_test_orig),
                        'train_samples': len(X_train),
                        'test_samples': len(X_test)
                    }
                    
                    # RMSE
                    metrics['train_rmse'] = np.sqrt(metrics['train_mse'])
                    metrics['test_rmse'] = np.sqrt(metrics['test_mse'])
                    
                    # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –æ—à–∏–±–∫–∏ –¥–ª—è —Ü–µ–Ω–æ–≤—ã—Ö —Ü–µ–ª–µ–π
                    if 'close' in target_name or 'price' in target_name:
                        metrics['test_mape'] = np.mean(
                            np.abs((y_test_orig - y_pred_test_orig) / y_test_orig)
                        ) * 100
                    
                    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
                    if validate and len(X) > 100:
                        try:
                            cv_scores = cross_val_score(
                                self.models[target_name], X_scaled, y_scaled, 
                                cv=5, scoring='r2'
                            )
                            metrics['cv_r2_mean'] = cv_scores.mean()
                            metrics['cv_r2_std'] = cv_scores.std()
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è {target_name}: {e}")
                            metrics['cv_r2_mean'] = 0.0
                            metrics['cv_r2_std'] = 0.0
                    
                    all_metrics[target_name] = metrics
                    
                    logger.info(f"‚úÖ {target_name}: R¬≤ = {metrics['test_r2']:.3f}, "
                               f"RMSE = {metrics['test_rmse']:.3f}")
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {target_name}: {e}")
                    all_metrics[target_name] = {'error': str(e)}
            
            training_time = (datetime.now() - start_time).total_seconds()
            
            # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            summary_metrics = {
                'models_trained': len([m for m in all_metrics.values() if 'error' not in m]),
                'total_models': len(self.models),
                'training_time': training_time,
                'total_samples': len(X),
                'features_count': len(feature_columns),
                'models_metrics': all_metrics,
                'timestamp': datetime.now().isoformat()
            }
            
            # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            valid_metrics = [m for m in all_metrics.values() if 'error' not in m]
            if valid_metrics:
                summary_metrics['average_r2'] = np.mean([m['test_r2'] for m in valid_metrics])
                summary_metrics['average_rmse'] = np.mean([m['test_rmse'] for m in valid_metrics])
                summary_metrics['average_mae'] = np.mean([m['test_mae'] for m in valid_metrics])
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            self.performance_metrics = summary_metrics
            self.is_fitted = True
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            self.training_history.append({
                'timestamp': datetime.now(),
                'metrics': summary_metrics,
                'model_type': self.model_type,
                'samples_count': len(X)
            })
            
            logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –°—Ä–µ–¥–Ω–∏–π R¬≤: {summary_metrics.get('average_r2', 0):.3f}")
            
            return summary_metrics
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return {'error': str(e)}
    
    def predict(self, features_df: pd.DataFrame) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω–æ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π
        
        Args:
            features_df: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        """
        if not self.is_fitted:
            logger.error("‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –æ–±—É—á–µ–Ω—ã")
            return {'error': 'models_not_fitted'}
        
        if not SKLEARN_AVAILABLE:
            logger.error("‚ùå scikit-learn –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return {'error': 'sklearn_unavailable'}
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            missing_features = set(self.feature_names) - set(features_df.columns)
            if missing_features:
                logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
                return {'error': f'missing_features: {missing_features}'}
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X = features_df[self.feature_names].copy()
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
            valid_indices = ~X.isnull().any(axis=1)
            if not valid_indices.any():
                logger.error("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
                return {'error': 'no_valid_data'}
            
            X_valid = X[valid_indices]
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if self.scalers['features']:
                X_scaled = self.scalers['features'].transform(X_valid)
            else:
                X_scaled = X_valid.values
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
            predictions = {}
            
            for target_name, model in self.models.items():
                try:
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    y_pred_scaled = model.predict(X_scaled)
                    
                    # –û–±—Ä–∞—Ç–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                    if target_name in self.scalers['targets']:
                        y_pred = self.scalers['targets'][target_name].inverse_transform(
                            y_pred_scaled.reshape(-1, 1)
                        ).ravel()
                    else:
                        y_pred = y_pred_scaled
                    
                    predictions[target_name] = y_pred.tolist()
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è {target_name}: {e}")
                    predictions[target_name] = {'error': str(e)}
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            results = {
                'predictions': predictions,
                'forecast_horizon': self.forecast_horizon,
                'valid_samples': len(X_valid),
                'total_samples': len(X),
                'valid_indices': valid_indices.tolist(),
                'timestamp': datetime.now().isoformat(),
                'model_type': self.model_type
            }
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if 'future_close' in predictions and isinstance(predictions['future_close'], list):
                current_prices = features_df['close'][valid_indices].values
                future_prices = np.array(predictions['future_close'])
                
                # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
                price_changes = (future_prices / current_prices - 1) * 100
                results['price_change_percent'] = price_changes.tolist()
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                results['prediction_stats'] = {
                    'mean_price_change': float(np.mean(price_changes)),
                    'std_price_change': float(np.std(price_changes)),
                    'min_price_change': float(np.min(price_changes)),
                    'max_price_change': float(np.max(price_changes))
                }
            
            logger.info(f"‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è {len(predictions)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
            logger.info(f"üìä –í–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(X_valid)}/{len(X)}")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return {'error': str(e)}
    
    def predict_single(self, features: Union[pd.Series, Dict, np.ndarray]) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            features: –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
        """
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
            if isinstance(features, dict):
                features_df = pd.DataFrame([features])
            elif isinstance(features, pd.Series):
                features_df = features.to_frame().T
            elif isinstance(features, np.ndarray):
                features_df = pd.DataFrame([features], columns=self.feature_names)
            else:
                features_df = pd.DataFrame([features])
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            result = self.predict(features_df)
            
            if 'error' in result:
                return result
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
            single_predictions = {}
            for target_name, pred_list in result['predictions'].items():
                if isinstance(pred_list, list) and len(pred_list) > 0:
                    single_predictions[target_name] = pred_list[0]
                else:
                    single_predictions[target_name] = pred_list
            
            single_result = {
                'predictions': single_predictions,
                'forecast_horizon': self.forecast_horizon,
                'timestamp': result['timestamp']
            }
            
            return single_result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞: {e}")
            return {'error': str(e)}
    
    def get_model_performance(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        return self.performance_metrics
    
    def get_feature_importance(self, target_name: str = None) -> pd.DataFrame:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            target_name: –ò–º—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–µ—Å–ª–∏ None, —Ç–æ –¥–ª—è –≤—Å–µ—Ö)
        """
        if not self.is_fitted:
            logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª–∏ –Ω–µ –æ–±—É—á–µ–Ω—ã")
            return pd.DataFrame()
        
        try:
            if target_name and target_name in self.models:
                # –í–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
                model = self.models[target_name]
                if hasattr(model, 'feature_importances_'):
                    return pd.DataFrame({
                        'feature': self.feature_names,
                        'importance': model.feature_importances_,
                        'target': target_name
                    }).sort_values('importance', ascending=False)
            else:
                # –í–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
                all_importance = []
                for target_name, model in self.models.items():
                    if hasattr(model, 'feature_importances_'):
                        for i, feature in enumerate(self.feature_names):
                            all_importance.append({
                                'feature': feature,
                                'importance': model.feature_importances_[i],
                                'target': target_name
                            })
                
                if all_importance:
                    df = pd.DataFrame(all_importance)
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
                    avg_importance = df.groupby('feature')['importance'].mean().reset_index()
                    return avg_importance.sort_values('importance', ascending=False)
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return pd.DataFrame()
    
    def save_models(self, directory: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            Path(directory).mkdir(parents=True, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å –æ—Ç–¥–µ–ª—å–Ω–æ
            for target_name, model in self.models.items():
                model_path = Path(directory) / f"{target_name}_model.joblib"
                joblib.dump(model, model_path)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–∞–ª–µ—Ä—ã
            scalers_path = Path(directory) / "scalers.joblib"
            joblib.dump(self.scalers, scalers_path)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'model_type': self.model_type,
                'forecast_horizon': self.forecast_horizon,
                'price_targets': self.price_targets,
                'feature_names': self.feature_names,
                'performance_metrics': self.performance_metrics,
                'is_fitted': self.is_fitted,
                'timestamp': datetime.now().isoformat()
            }
            
            metadata_path = Path(directory) / "metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {directory}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
    
    def load_models(self, directory: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            directory = Path(directory)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata_path = directory / "metadata.json"
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            
            self.model_type = metadata['model_type']
            self.forecast_horizon = metadata['forecast_horizon']
            self.price_targets = metadata['price_targets']
            self.feature_names = metadata['feature_names']
            self.performance_metrics = metadata['performance_metrics']
            self.is_fitted = metadata['is_fitted']
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∫–∞–ª–µ—Ä—ã
            scalers_path = directory / "scalers.joblib"
            self.scalers = joblib.load(scalers_path)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
            self.models = {}
            for target_name in self.price_targets:
                model_path = directory / f"{target_name}_model.joblib"
                if model_path.exists():
                    self.models[target_name] = joblib.load(model_path)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {directory}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö"""
        return {
            'model_type': self.model_type,
            'forecast_horizon': self.forecast_horizon,
            'price_targets': self.price_targets,
            'scaling_method': self.scaling_method,
            'is_fitted': self.is_fitted,
            'features_count': len(self.feature_names),
            'models_count': len(self.models),
            'feature_names': self.feature_names,
            'performance_metrics': self.performance_metrics,
            'training_history_count': len(self.training_history),
            'sklearn_available': SKLEARN_AVAILABLE
        }

# ‚úÖ –≠–ö–°–ü–û–†–¢
__all__ = [
    'PriceLevelRegressor'
]